{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corpus metadata (required for breaking the corpus into the desire size documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maptable = pd.read_csv(\"data/block-item-toplevel-map.csv\")\n",
    "maptable[[\"block\", \"item\", \"toplevel\"]] = maptable[[\"block\", \"item\", \"toplevel\"]].apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_item_dict = dict(zip(maptable['block'], maptable['item']))\n",
    "item_toplevel_dict = dict(zip(maptable['item'], maptable['toplevel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aamaptable = pd.read_csv(\"data/aquinasSTandScriptumArticleBlocks.csv\")\n",
    "aamaptable[[\"block\", \"article\", \"toplevel\"]] = aamaptable[[\"block\", \"article\", \"toplevel\"]].apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aablock_item_dict = dict(zip(aamaptable['block'], aamaptable['article']))\n",
    "aaitem_toplevel_dict = dict(zip(aamaptable['article'], aamaptable['toplevel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"data/alltext.csv\")\n",
    "toplevellist = text[\"topLevel\"].str.strip().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize(row):\n",
    "    try:\n",
    "        #strip punctuation\n",
    "        re_stripper_alpha = re.compile('[^a-zA-Z]+')\n",
    "        #reduce white space\n",
    "        newText = re_stripper_alpha.sub(' ', row[\"text\"])\n",
    "        #conversions\n",
    "        newText = newText.lower()\n",
    "        newText = newText.replace(\"ae\", \"e\")\n",
    "        newText = newText.replace(\"v\", \"u\")\n",
    "        newText = newText.replace(\"j\", \"i\")\n",
    "        newText = newText.replace(\"y\", \"i\")\n",
    "        newText = newText.replace(\"oe\", \"e\")\n",
    "        return newText\n",
    "    \n",
    "    except Exception:\n",
    "        return \"\"\n",
    "text[\"text_clean\"] = text.apply(lambda x: text_normalize(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide corpus into \"Item\" sized documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDocsDict(block_text_dict):\n",
    "  documents_dict = {}\n",
    "  for key, value in block_text_dict.items():\n",
    "    # exclude list is to exclude some texts that are not latin and are an experimental part of corpus\n",
    "    excludeList = [\"UD1xh4-\", \"ee-\", \"dor5dc-\"]\n",
    "    # run conditional to exclude texts that begin with the above prefix\n",
    "    if not any(key.startswith(prefix) for prefix in excludeList):\n",
    "      if (key.startswith(\"TAca84-\") or key.startswith(\"ta-\")):\n",
    "        if aablock_item_dict.get(key):\n",
    "          itemid = aablock_item_dict.get(key)\n",
    "          current_value = documents_dict.get(itemid)\n",
    "          if current_value:\n",
    "              documents_dict[itemid] = current_value + ' ' + value\n",
    "          else:\n",
    "              documents_dict[itemid] = value\n",
    "      else:\n",
    "        if block_item_dict.get(key):\n",
    "          itemid = block_item_dict.get(key)\n",
    "          current_value = documents_dict.get(itemid)\n",
    "          if current_value:\n",
    "              documents_dict[itemid] = current_value + ' ' + value\n",
    "          else:\n",
    "              documents_dict[itemid] = value\n",
    "  return documents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_text_dict = dict(zip(text['id'], text['text_clean']))\n",
    "documents_dict = createDocsDict(block_text_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dictionaries to look up documents by index number or by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents2label = {}\n",
    "label2documents = {}\n",
    "counter = 0\n",
    "for doc in documents_dict:\n",
    "    documents2label[counter] = doc\n",
    "    label2documents[doc] = counter\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(documents_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Gensim Tagged Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_corpus():\n",
    "  #for i, row in text.iterrows():\n",
    "  for idx, val in enumerate(documents):\n",
    "    tokens = gensim.utils.simple_preprocess(val)\n",
    "    # add genesim tags\n",
    "    yield gensim.models.doc2vec.TaggedDocument(tokens, [idx])\n",
    "\n",
    "# tagging can take about a minute\n",
    "gensim_tagged_corpus = list(tag_corpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model (either from saved model or new training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode():\n",
    "  model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=10, epochs=100)\n",
    "  model.build_vocab(gensim_tagged_corpus)\n",
    "  model.train(gensim_tagged_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train=False):\n",
    "  if train:\n",
    "    model = train_mode()\n",
    "  else:\n",
    "    model = Doc2Vec.load(\"../SCTACorpus-doc2vec.model\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportMatchResult(model, doc_id, target_doc_id):\n",
    "  inferred_vector = model.infer_vector(gensim_tagged_corpus[doc_id].words)\n",
    "  sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "  filteredSims = [t for t in sims if documents2label[t[0]].startswith(documents2label[target_doc_id].split(\"-\")[0])]\n",
    "  resultFiltered = [i for i,t in enumerate(filteredSims) if t[0] == target_doc_id][0]\n",
    "  resultNotFiltered = [i for i,t in enumerate(sims) if t[0] == target_doc_id][0]\n",
    "  resultPerc = [t[1] for i,t in enumerate(sims) if t[0] == target_doc_id][0]\n",
    "  return (resultFiltered, resultNotFiltered, resultPerc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportTopResults(model, doc_id, topn=11):\n",
    "  inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "  sims = model.dv.most_similar([inferred_vector], topn=topn)\n",
    "  return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/aquinasSuggestedMatches.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n3/l9gl3dx13n5bywlfvjm7hpv80000gq/T/ipykernel_30070/101919029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this cell can take about 2 minutes to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatches4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/aquinasSuggestedMatches.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmatches6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/aquinasSuggestedMatchesV6.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmatches7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/aquinasSuggestedMatchesV7.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatches4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/aquinasSuggestedMatches.csv'"
     ]
    }
   ],
   "source": [
    "# this cell can take about 2 minutes to run\n",
    "matches4 = pd.read_csv(\"data/aquinasSuggestedMatchesV4.csv\")\n",
    "matches6 = pd.read_csv(\"data/aquinasSuggestedMatchesV6.csv\")\n",
    "matches7 = pd.read_csv(\"data/aquinasSuggestedMatchesV7.csv\")\n",
    "matches = pd.concat([matches4, matches6, matches7])\n",
    "\n",
    "results = {}\n",
    "for index, row in matches.iterrows():\n",
    "  if (index >= 0):  \n",
    "    source = row[\"ST\"]\n",
    "    target = row[\"Match\"]\n",
    "    note = str(row[\"note\"])\n",
    "    if \"target\" not in note:\n",
    "      result = reportMatchResult(model, label2documents[source], label2documents[target])\n",
    "      key = source + \"===\" + target\n",
    "      results[key] = result\n",
    "  \n",
    "resultsdf = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"matchFiltered\", \"matchUnfiltered\", \"matchPerc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "Q1 = resultsdf.quantile(0.0)\n",
    "Q3 = resultsdf.quantile(0.90)\n",
    "IQR = Q3 - Q1\n",
    "# Remove outliers from each column\n",
    "maskdf = resultsdf[~(resultsdf > (Q3 + 1.5 * IQR)).any(axis=1)]\n",
    "\n",
    "# Calculate the mean of each column\n",
    "mean_values = maskdf.mean()\n",
    "median_values = maskdf.median()\n",
    "\n",
    "# Add the mean values as a new row to the DataFrame\n",
    "resultsdf.loc[\"mean\"] = mean_values\n",
    "resultsdf.loc[\"median\"] = median_values\n",
    "\n",
    "resultsdf.tail(11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
